{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0de0589",
   "metadata": {},
   "source": [
    "# Key and Meter Estimation (100 points in total)\n",
    "\n",
    "This is a template for your report. The report should be written like a blog post and describe what you did for the alignment challenge."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3eb98e2b",
   "metadata": {},
   "source": [
    "## Key Estimation (50 points)\n",
    "\n",
    "### Problem Description\n",
    "* What is key estimation and tonality?\n",
    "\n",
    "Key estimation is the process of identifying the key of a musical piece. In Western music, the key of a piece is the tonal center around which the harmonies and melodies of the piece are built. A key is typically specified using a combination of a note name (such as C or F#) and a mode (such as major or minor), and it determines the scale and chord progression used in the piece.\n",
    "\n",
    "Tonality is the quality of a piece of music that is determined by its key. In tonal music, the notes of the scale and the chords used in the piece are arranged in a hierarchical structure that creates a sense of resolution and stability. In other words, tonality is the system of relationships between the notes and chords used in a piece of music, and it is determined by the key of the piece.\n",
    "\n",
    "* Why is this an interesting problem in music processing?\n",
    "\n",
    "Key estimation is an interesting problem in music processing because it can help computers better understand and analyze the structure of a piece of music. By identifying the key of a piece, a computer can gain insights into its harmonic and melodic structure, which can be useful for tasks such as automatic music transcription and music generation. Additionally, key estimation can help computers better align music with other data, such as lyrics or visualizations. For example, a key estimation system could be used to ensure that a musical piece is synchronized with a video or to automatically generate a musical accompaniment for a given vocal melody. In general, key estimation is an important step in the broader goal of enabling computers to understand and generate music in a way that is similar to how humans do.\n",
    "\n",
    "### Feature Description\n",
    "\n",
    "* Which features did you use?\n",
    "\n",
    "key: Key of the piece (as a string, e.g. C, C#m, etc.) \\\n",
    "ts_num: Numerator of the time signature. \\\n",
    "ts_denom: Denominator of the time signature. \\\n",
    "tempo(bpm): The tempo of the piece in beats per minute.\n",
    "\n",
    "* Which elements of the music do they capture?\n",
    "\n",
    "These features capture different aspects of the music and can provide information about its structure and organization. The key of the piece captures the tonal center of the piece and the mode (major or minor) in which it is written, while the time signature and tempo capture the overall rhythmic structure of the piece. Together, these features can provide information about the harmonic, melodic, and rhythmic content of the piece, which can be useful for key estimation.\n",
    "\n",
    "* Was there any pre-processing of the features?\n",
    "\n",
    "Yes, the key_identification function pre-processes the MIDI file by extracting the pitch class distribution from the file. This is done by first loading the MIDI file into a PianoRoll object using the partitura library, and then computing the pitch class distribution by summing the piano roll over the time dimension. The pitch class distribution is then used as input to the KeyProfileObservationModel to compute the likelihood of the observed pitch class distribution given the keys.\n",
    "\n",
    "* Anything that you think relevant.\n",
    "\n",
    "In addition to extracting the pitch class distribution, the key_identification function also computes the time signature and tempo of the piece by accessing the relevant metadata in the PianoRoll object. These features are then used as input to the HMM along with the pitch class distribution to make the key prediction. The function uses a default key profile matrix, inertia parameter, piano roll resolution, and window size for the HMM, but these can be customized by providing different values for the corresponding arguments when calling the function.\n",
    "\n",
    "* Make sure to include plots and visualizations!\n",
    "\n",
    "### Methods\n",
    "\n",
    "* Which Method(s) did you use? (e.g., HMM, Krumhansl-Schmuckler, etc.)\n",
    "\n",
    " hidden Markov model (HMM) \n",
    "\n",
    "* What is this method supposed to do (conceptually, you don't need to go into an in-depth description of the methods)\n",
    "\n",
    "The HMM is composed of two parts: a transition model that describes the probability of moving from one state to another, and an observation model that describes the probability of observing a particular observation given the current state. In the context of the key estimation challenge, the HMM is used to model the sequence of pitch class distributions in a piece of music, and the internal states of the HMM correspond to the possible keys. The transition model describes the likelihood of transitioning from one key to another, and the observation model describes the likelihood of observing a particular pitch class distribution given a particular key. The HMM is trained on a dataset of labeled music pieces, and then used to make predictions on unseen pieces by finding the most likely sequence of keys given the observed pitch class distributions.\n",
    "\n",
    "* What were the hyper-parameters/settings of the method?\n",
    "\n",
    "The hyperparameters of the HMM method used in the code you provided are the key profile matrix, the inertia parameter, and the window size.\n",
    "\n",
    "The key profile matrix is a 24 x 12 matrix that describes the pitch class profiles for each of the 24 possible keys. The code uses the \"kp\" key profile matrix, which is a matrix of probabilities representing the likelihood of observing each pitch class in a given key. The inertia parameter determines how likely it is that the HMM will stay in the same key from one time step to the next. The window size determines the length of the window over which the pitch class distribution is computed for each time step.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d1ba25c3",
   "metadata": {},
   "source": [
    "## Meter Estimation (50 points)\n",
    "\n",
    "### Problem Description\n",
    "* What is meter estimation and bpm?\n",
    "\n",
    "Meter estimation is the process of identifying the meter, or rhythmic structure, of a piece of music. In music, meter is typically indicated by the time signature, which specifies the number of beats per measure and the type of note that represents one beat. The tempo, or speed, of a piece of music is typically measured in beats per minute (bpm), which indicates how many beats are played in one minute. The combination of meter and tempo give a piece of music its rhythmic character.\n",
    "\n",
    "* Why is this an interesting problem in music processing?\n",
    "\n",
    "Meter estimation is an important problem in music processing because it is a fundamental aspect of musical structure and can have a significant impact on how a piece of music is perceived and performed. By accurately estimating the meter of a piece of music, a machine learning model can help musicians and composers better understand and analyze the structure of a piece, and potentially even assist in the creation of new music. Additionally, accurately estimating the tempo (or beats per minute) of a piece can help with synchronization and alignment of different audio tracks in a performance.\n",
    "\n",
    "### Feature Description\n",
    "\n",
    "* Which features did you use?\n",
    "\n",
    "The code uses the onsets and saliences of notes in the music piece to estimate the meter and tempo (in beats per minute).\n",
    "\n",
    "* Which elements of the music do they capture?\n",
    "\n",
    "The features used in the above code are onsets, onsets duration, and the salience of notes. These features capture the timing and the importance of notes in a piece of music. The onsets and onsets duration capture the timing of the notes, while the salience of notes captures how important each note is in the piece. \n",
    "\n",
    "* Was there any pre-processing of the features?\n",
    "\n",
    "It appears that the code takes in a MIDI file and performs some pre-processing steps, including aggregating notes in clusters, to extract features from the performance data. These features are then used to estimate the time signature numerator and the tempo of the piece.\n",
    "\n",
    "* Anything that you think relevant.\n",
    "\n",
    "In the code, the pre-processing of the features involves aggregating the onsets (i.e. the timestamps of the notes) into clusters of notes that are played within a certain time window (specified by the CHORD_SPREAD_TIME constant). This is done to avoid making assumptions about the exact timing of notes played simultaneously and to improve the accuracy of the meter estimation.\n",
    "\n",
    "* Make sure to include plots and visualizations!\n",
    "\n",
    "### Methods\n",
    "\n",
    "* Which Method(s) did you use? (e.g., HMM, Krumhansl-Schmuckler, etc.)\n",
    "\n",
    "The Multiple Agents method\n",
    "\n",
    "* What is this method supposed to do (conceptually, you don't need to go into an in-depth description of the methods)\n",
    "\n",
    "The method used in the code is MultipleAgents from the partitura library, which is a method for estimating the meter and tempo of a piece of music from a sequence of onsets and saliences. The onsets represent the timestamps at which notes start, and the saliences represent how prominent or loud the notes are. The MultipleAgents method uses this information to estimate the meter (time signature numerator) and tempo (beats per minute) of the piece.\n",
    "\n",
    "* What were the hyper-parameters/settings of the method?\n",
    "\n",
    "The hyper-parameters/settings of the method were the inertia_param and piano_roll_resolution parameters. The inertia_param is a value between 0 and 1 that indicates how likely it is that the same key will be repeated. The piano_roll_resolution parameter specifies the number of pitches that are considered in the piano roll.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0504195f",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "Reference the papers used for this work!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
